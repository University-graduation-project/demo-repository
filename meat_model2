import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# Paths to dataset
train_dir = "/home/daniah/Desktop/ExternalDrive/meat_data/train"
validation_dir = "/home/daniah/Desktop/ExternalDrive/meat_data/validation"

# Load datasets
train_dataset = tf.keras.utils.image_dataset_from_directory( #load your img from folder and preper them for training
    train_dir, #where the data is
    image_size = (150, 150), #resize all img so i ensures cons input size (for neural), i use smaller size for faster training 
    batch_size = 32, # process 32 img at once (for faster training and for memory) also helps the model generalize better(will learn from groups of img)
    label_classification = 'binary' # 0 for spoiled and 1 for fresh , no manual label needed
)

validation_dataset = tf.keras.utils.image_dataset_from_directory(
    validation_dir,
    image_size = (150, 150),
    batch_size = 32,
    label_classification = 'binary'
)

#data increase (dont forget must be before cnn) / i did it cuz if the blood on right it will now that is spoiled if it was in left no so it could learn that blood no matter where = spoiled
data_increase = tf.keras.Sequential([ 
    layers.RandomFlip("horizontal"), #flip img left/right (i trained the model that if change it doesn't matter and i double the training data(org + flipped))
    layers.Rescaling(1./255) #normaliz pixel to 0 and 1 (Divides all pixel values by 255 (e.g., a pixel with value 150 becomes 150/255 â‰ˆ 0.59), Neural networks train faster when inputs are small numbers ([0, 1] instead of [0, 255]).
]) #i can do rotation and zoom also for more data but it worked so i made it simple :)

# Model architecture (My model BRAIN)
model = models.Sequential([
    data_increase, #apply img transformation first
    layers.Conv2D(32, (3, 3), activation='relu'), #32 filter, 3x3 size (Detects patterns (edges, spots, discoloration)	cuz Meat freshness depends on visual features)
    layers.MaxPooling2D((2, 2)), Reduces image size while keeping important features
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(), #Converts image data into a simple list	cuz Prepares for standard neural network layers
    layers.Dense(128, activation='relu'), #Learns relationships between detected patterns	cuz Combines features to make decisions
    layers.Dropout(0.2),  # Helps prevent overfitting /Randomly turns off neurons during training	cuz Prevents overfitting (memorizing training images)
    layers.Dense(1, activation='sigmoid') #output 0 and 1
])

# Compile
model.compile(
    optimizer = 'adam', #
    loss = 'binary_crossentropy',
    metrics = ['accuracy']
)

# Early stopping to prevent overtraining (from yours model)
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=2,
    restore_best_weights=True
)

# Train
history = model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=10,
    callbacks=[early_stop]
)

# Save model
model.save('Meat_Model.keras')

# Plot results
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

plt.tight_layout()
plt.savefig('training_results.png')
plt.close()

print("Training complete!")
print(f"Best validation accuracy: {max(history.history['val_accuracy']):.2%}")
